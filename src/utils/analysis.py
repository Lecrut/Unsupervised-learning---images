"""
Funkcje do analizy przestrzeni latentnej i klasteryzacji.
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, adjusted_rand_score
from torch.utils.data import DataLoader
import umap
from typing import Tuple, Optional


def extract_latent_vectors(model: torch.nn.Module,
                          dataloader: DataLoader,
                          device: torch.device,
                          max_samples: Optional[int] = None) -> np.ndarray:
    """
    Ekstraktuje wektory latentne z trenowanego autoencodera.
    
    Args:
        model: trenowany autoencoder
        dataloader: DataLoader z danymi
        device: urzƒÖdzenie (cuda/cpu)
        max_samples: maksymalna liczba pr√≥bek (None = wszystkie)
        
    Returns:
        numpy array z wektorami latentnymi [n_samples, latent_dim]
    """
    model.eval()
    latent_vectors = []
    samples_processed = 0
    
    print(f"üîç Ekstraktowanie wektor√≥w latentnych...")
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(dataloader):
            if max_samples and samples_processed >= max_samples:
                break
            
            # Obs≈Çuga r√≥≈ºnych format√≥w danych
            if isinstance(batch, (list, tuple)) and len(batch) >= 2:
                imgs = batch[0]  # Pierwszy element to obrazy
            else:
                imgs = batch
                
            imgs = imgs.to(device)
            
            # Ekstraktuj reprezentacjƒô latentnƒÖ
            if hasattr(model, 'encode'):
                z = model.encode(imgs)
            else:
                _, z = model(imgs)  # Zak≈Çadamy ≈ºe model zwraca (output, latent)
            
            latent_vectors.append(z.cpu().numpy())
            samples_processed += imgs.size(0)
            
            if (batch_idx + 1) % 10 == 0:
                print(f"  Przetworzono {samples_processed} pr√≥bek...")

    latent_array = np.concatenate(latent_vectors, axis=0)
    
    if max_samples:
        latent_array = latent_array[:max_samples]
    
    print(f"‚úÖ Wyekstraktowano {latent_array.shape[0]} wektor√≥w latentnych "
          f"o wymiarze {latent_array.shape[1]}")
    
    return latent_array


def cluster_latent_space(latent_vectors: np.ndarray,
                        n_clusters: int = 10,
                        algorithm: str = 'kmeans',
                        random_state: int = 42,
                        **kwargs) -> Tuple[np.ndarray, dict]:
    """
    Klasteryzuje wektory latentne.
    
    Args:
        latent_vectors: array z wektorami latentnymi
        n_clusters: liczba klastr√≥w
        algorithm: algorytm klasteryzacji ('kmeans', 'spectral', 'dbscan', 'gaussian_mixture')
        random_state: seed dla reprodukowalno≈õci
        **kwargs: dodatkowe argumenty dla algorytmu
        
    Returns:
        Tuple: (etykiety_klastr√≥w, s≈Çownik_z_metrykami)
    """
    print(f"üß† Klasteryzacja {algorithm.upper()} na {n_clusters} klastr√≥w...")
    
    if algorithm == 'kmeans':
        from sklearn.cluster import KMeans
        clusterer = KMeans(
            n_clusters=n_clusters, 
            random_state=random_state, 
            n_init=kwargs.get('n_init', 10)
        )
    elif algorithm == 'spectral':
        from sklearn.cluster import SpectralClustering
        clusterer = SpectralClustering(
            n_clusters=n_clusters, 
            random_state=random_state,
            affinity=kwargs.get('affinity', 'rbf')
        )
    elif algorithm == 'dbscan':
        from sklearn.cluster import DBSCAN
        clusterer = DBSCAN(
            eps=kwargs.get('eps', 0.5), 
            min_samples=kwargs.get('min_samples', 5)
        )
    elif algorithm == 'gaussian_mixture':
        from sklearn.mixture import GaussianMixture
        clusterer = GaussianMixture(
            n_components=n_clusters,
            random_state=random_state,
            covariance_type=kwargs.get('covariance_type', 'full')
        )
    else:
        raise ValueError(f"Nieznany algorytm: {algorithm}")
    
    labels = clusterer.fit_predict(latent_vectors)
    
    # Oblicz metryki
    metrics = {}
    
    if len(set(labels)) > 1:  # Sprawd≈∫ czy sƒÖ r√≥≈ºne klastry
        # Silhouette score (tylko dla non-outlier punkt√≥w w DBSCAN)
        if algorithm == 'dbscan':
            mask = labels != -1
            if np.sum(mask) > 1 and len(set(labels[mask])) > 1:
                metrics['silhouette_score'] = silhouette_score(latent_vectors[mask], labels[mask])
            metrics['n_noise_points'] = np.sum(labels == -1)
        else:
            metrics['silhouette_score'] = silhouette_score(latent_vectors, labels)
        
        metrics['n_clusters_found'] = len(set(labels))
        
        # Davies-Bouldin Index (ni≈ºsza = lepsza)
        if algorithm != 'dbscan' or np.sum(labels != -1) > 0:
            from sklearn.metrics import davies_bouldin_score
            try:
                if algorithm == 'dbscan':
                    mask = labels != -1
                    metrics['davies_bouldin'] = davies_bouldin_score(latent_vectors[mask], labels[mask])
                else:
                    metrics['davies_bouldin'] = davies_bouldin_score(latent_vectors, labels)
            except:
                pass
        
        # Calinski-Harabasz Index (wy≈ºsza = lepsza)
        from sklearn.metrics import calinski_harabasz_score
        try:
            if algorithm == 'dbscan':
                mask = labels != -1
                if len(set(labels[mask])) > 1:
                    metrics['calinski_harabasz'] = calinski_harabasz_score(latent_vectors[mask], labels[mask])
            else:
                metrics['calinski_harabasz'] = calinski_harabasz_score(latent_vectors, labels)
        except:
            pass
    
    print(f"üìä Znaleziono {len(set(labels))} klastr√≥w")
    if 'silhouette_score' in metrics:
        print(f"üìà Silhouette score: {metrics['silhouette_score']:.4f}")
    if 'davies_bouldin' in metrics:
        print(f"üìâ Davies-Bouldin: {metrics['davies_bouldin']:.4f}")
    if 'calinski_harabasz' in metrics:
        print(f"üìä Calinski-Harabasz: {metrics['calinski_harabasz']:.2f}")
    
    return labels, metrics


def reduce_dimensionality(latent_vectors: np.ndarray,
                         method: str = 'umap',
                         n_components: int = 2,
                         **kwargs) -> np.ndarray:
    """
    Redukuje wymiarowo≈õƒá wektor√≥w latentnych do wizualizacji.
    
    Args:
        latent_vectors: wektory latentne
        method: metoda redukcji ('umap', 'tsne', 'pca')
        n_components: docelowa liczba wymiar√≥w
        **kwargs: dodatkowe argumenty dla algorytmu
        
    Returns:
        Array z zredukowanymi wymiarami
    """
    print(f"üìâ Redukcja wymiarowo≈õci metodƒÖ {method.upper()} do {n_components}D...")
    
    if method == 'umap':
        import umap
        reducer = umap.UMAP(
            n_components=n_components,
            n_neighbors=kwargs.get('n_neighbors', 15),
            min_dist=kwargs.get('min_dist', 0.1),
            random_state=kwargs.get('random_state', 42)
        )
    elif method == 'tsne':
        from sklearn.manifold import TSNE
        reducer = TSNE(
            n_components=n_components,
            perplexity=kwargs.get('perplexity', 30),
            random_state=kwargs.get('random_state', 42)
        )
    elif method == 'pca':
        from sklearn.decomposition import PCA
        reducer = PCA(n_components=n_components)
    else:
        raise ValueError(f"Nieznana metoda: {method}")
    
    embedding = reducer.fit_transform(latent_vectors)
    print(f"‚úÖ Zredukowano z {latent_vectors.shape[1]}D do {embedding.shape[1]}D")
    
    return embedding


def cluster_and_visualize(latent_vectors: np.ndarray,
                         n_clusters: int = 10,
                         n_samples_viz: int = 2000,
                         experiment = None,
                         method: str = 'umap',
                         figsize: Tuple[int, int] = (12, 8)) -> Tuple[np.ndarray, np.ndarray, dict]:
    """
    Kompletna analiza: klasteryzacja + redukcja wymiarowo≈õci + wizualizacja.
    
    Args:
        latent_vectors: wektory latentne
        n_clusters: liczba klastr√≥w dla KMeans
        n_samples_viz: liczba pr√≥bek do wizualizacji
        experiment: obiekt CometML do logowania
        method: metoda redukcji wymiarowo≈õci
        figsize: rozmiar wykresu
        
    Returns:
        Tuple: (etykiety_klastr√≥w, embedding_2d, s≈Çownik_metryk)
    """
    # Ograniczenie liczby pr√≥bek dla wizualizacji
    viz_vectors = latent_vectors[:n_samples_viz] if len(latent_vectors) > n_samples_viz else latent_vectors
    
    # Klasteryzacja na wszystkich danych
    cluster_labels, cluster_metrics = cluster_latent_space(latent_vectors, n_clusters)
    
    # Redukcja wymiarowo≈õci dla wizualizacji
    embedding = reduce_dimensionality(viz_vectors, method=method)
    
    # Przygotuj dane do wizualizacji
    viz_labels = cluster_labels[:len(viz_vectors)]
    
    # Tworzenie wykresu
    plt.figure(figsize=figsize)
    
    # Scatter plot z kolorami klastr√≥w
    scatter = plt.scatter(
        embedding[:, 0], 
        embedding[:, 1], 
        c=viz_labels, 
        cmap='Spectral', 
        alpha=0.7,
        s=20
    )
    
    plt.colorbar(scatter, label='Numer klastra')
    plt.title(f'Klasteryzacja przestrzeni latentnej\n'
              f'{method.upper()} + KMeans (k={n_clusters})')
    plt.xlabel(f'{method.upper()} 1')
    plt.ylabel(f'{method.upper()} 2')
    
    # Dodaj statystyki na wykres
    if 'silhouette_score' in cluster_metrics:
        plt.text(0.02, 0.98, f"Silhouette: {cluster_metrics['silhouette_score']:.3f}", 
                transform=plt.gca().transAxes, fontsize=12, 
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    plt.tight_layout()
    
    # Logowanie do Comet ML
    if experiment:
        experiment.log_figure("latent_space_clustering", plt)
        experiment.log_metrics(cluster_metrics)
    
    plt.show()
    
    # Analiza klastr√≥w
    print("\nüìä Analiza klastr√≥w:")
    unique_labels, counts = np.unique(cluster_labels, return_counts=True)
    for label, count in zip(unique_labels, counts):
        percentage = count / len(cluster_labels) * 100
        print(f"  Klaster {label}: {count} pr√≥bek ({percentage:.1f}%)")
    
    return cluster_labels, embedding, cluster_metrics


def analyze_cluster_characteristics(latent_vectors: np.ndarray,
                                  cluster_labels: np.ndarray,
                                  feature_names: Optional[list] = None) -> dict:
    """
    Analizuje charakterystyki ka≈ºdego klastra w przestrzeni latentnej.
    
    Args:
        latent_vectors: wektory latentne
        cluster_labels: etykiety klastr√≥w
        feature_names: nazwy cech (opcjonalne)
        
    Returns:
        S≈Çownik z charakterystykami klastr√≥w
    """
    analysis = {}
    unique_labels = np.unique(cluster_labels)
    
    for label in unique_labels:
        mask = cluster_labels == label
        cluster_vectors = latent_vectors[mask]
        
        analysis[f'cluster_{label}'] = {
            'size': np.sum(mask),
            'percentage': np.sum(mask) / len(cluster_labels) * 100,
            'mean': np.mean(cluster_vectors, axis=0),
            'std': np.std(cluster_vectors, axis=0),
            'centroid': np.mean(cluster_vectors, axis=0)
        }
    
    return analysis